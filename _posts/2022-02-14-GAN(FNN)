---
layout: single
title:  "VAE"
categories: coding_vae
tag: [python] 
toc: true
author_profile: false #true
sidebar:
    nav: "docs"
search: true #false:검색기능/ 검색되지 않음
---
## GAN 구현하기

###  GAN 모델구성


```python
# 라이브러리 정의
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU 
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

# 모델 차원정의
img_rows = 28
img_cols = 28
channels = 1
z_dim = 100

img_shape = (img_cols, img_rows, channels)

# 구성요소: 생성자
def build_generator(img_shape, z_dim):
    model = Sequential()
    model.add(Dense(128, input_dim=z_dim))
    model.add(LeakyReLU(alpha=0.01))
    model.add(Dense(28*28*1, activation='tanh'))
    model.add(Reshape(img_shape))
    return model

# 구성요소: 판별자
def build_discriminator(img_shape):
    model = Sequential()
    model.add(Flatten(input_shape=img_shape))
    model.add(Dense(128))
    model.add(LeakyReLU(alpha = 0.01))
    model.add(Dense(1, activation='sigmoid'))
    return model
    

# GAN 모델구성
def build_gan(generator, discriminator):
    
    model = Sequential() 
    
    #생성자 -> 판별자로 연결
    model.add(generator)
    model.add(discriminator)
    return model



# 판별자 모델 만들고 컴파일하기
discriminator = build_discriminator(img_shape)
discriminator.compile(loss='binary_crossentropy', optimizer = Adam(), metrics=['accuracy'])


# 생성자 모델 만들기
generator = build_generator(img_shape, z_dim)

# 생성자 훈련하는 동안 판별자의 파라미터를 유지
discriminator.trainable = False

# 생성자를 훈련하기 위해 동결된 판별자로 GAN모델 만들고 컴파일
gan =build_gan(generator, discriminator)
gan.compile(loss='binary_crossentropy', optimizer=Adam())










```

### GAN 반복 훈련


```python
# 학습시간 정의
import time
from datetime import timedelta
start = time.process_time()

losses = []
accuracies = []
iteration_checkpoints = []


# (x_train, _), (_,_) = mnist.load_data()


def train(iterations, batch_size, sample_interval):
    
    (x_train, _), (_,_) = mnist.load_data()
    
    # [0, 255] 흑백 필셀 값을 [-1, 1]로 스케일 조정
    x_train = (x_train / 127.5) -1.0   
    x_train = np.expand_dims(x_train, axis=3)
    
    # 진짜 이미지 레이블 모두:1
    real= np.ones((batch_size, 1))
    # 가짜 이미지 레이블 모두:0
    fake= np.zeros((batch_size, 1))
    
    for iteration in range(iterations):
        
        #---------------------------------- 판별자 훈련 ----------------------------------#
        
        # 진짜 이미지에서 랜덤 배치 가져오기
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        imgs = x_train[idx]
        
        # 가짜 이미지 배치 생성
        z = np.random.normal(0,1,(batch_size,100)) #(0[평균],1[표준편차],[(size)])
        gen_imgs = generator.predict(z)
        
        # 판별자 훈련
        d_loss_real = discriminator.train_on_batch(imgs, real)
        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
        d_loss, accuracy = 0.5*np.add(d_loss_real, d_loss_fake)
        
        
        #---------------------------------- 생성자 훈련 ----------------------------------#
        # 가짜 이미지 배치 생성
        z = np.random.normal(0, 1 , (batch_size, 100))
        gen_imgs = generator.predict(z)
        
        # 생성자 훈련
        g_loss = gan.train_on_batch(z, real)
        
        if (iteration + 1) % sample_interval == 0:
            
            # 훈련이 끝난 뒤 그래프 그리기 위해 손실 , 정확도 저장
            losses.append((d_loss, g_loss))
            accuracies.append(100.0*accuracy)
            iteration_checkpoints.append(iteration+1)
            
            # 훈련과정 출력
            print("%d [D 손실: %f, 정확도: %.2f][G 손실: %f]" %(iteration+1, d_loss, 100*accuracy, g_loss))
            
            # 생성된 이미지 샘플출력
            sample_images(generator)
    
    
    
    

```

### 샘플 이미지 출력


```python
def sample_images(generator, image_grid_rows=4, image_grid_columns=4):
    
    # 랜덤 잡음 샘플링
    z = np.random.normal(0,1, (image_grid_rows*image_grid_columns, z_dim))
    
    # 랜덤한 잡음에서 이미지 생성하기 
    gen_imgs = generator.predict(z)
    
    # 이미지 픽셀 값을 [0,1]사이로 스케일 조정
    gen_imgs = 0.5*gen_imgs+0.5
    
    # 이미지 그리드 설정
    fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4,4), sharey=True, sharex=True)
    
    cnt =0 
    for i in range(image_grid_rows):
        for j in range(image_grid_columns):
            
            # 이미지 그리드 출력
            axs[i, j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')
            axs[i, j].axis('off')
            cnt +=1
            
```

### 학습수행


```python
# 하이퍼파라미터 설정
iterations = 20000
batch_size = 128
sample_interval = 1000


train(iterations, batch_size, sample_interval)


# 학습시간계산
end = time.process_time()   
print("Time elapsed:{:.2f} sec " .format(end - start))  # seconds
print("Time elapsed:{:.2f} min " .format((end - start)/60)) # min
```

    1000 [D 손실: 0.089168, 정확도: 98.83][G 손실: 3.636932]
    2000 [D 손실: 0.158443, 정확도: 96.09][G 손실: 4.016033]
    3000 [D 손실: 0.267733, 정확도: 91.41][G 손실: 5.064198]
    4000 [D 손실: 0.192038, 정확도: 92.58][G 손실: 4.153424]
    5000 [D 손실: 0.189237, 정확도: 94.14][G 손실: 5.254642]
    6000 [D 손실: 0.424051, 정확도: 85.55][G 손실: 5.639934]
    7000 [D 손실: 0.118361, 정확도: 96.09][G 손실: 4.563922]
    8000 [D 손실: 0.353240, 정확도: 85.94][G 손실: 4.394133]
    9000 [D 손실: 0.395582, 정확도: 80.86][G 손실: 3.326217]
    10000 [D 손실: 0.536611, 정확도: 75.78][G 손실: 2.843086]
    11000 [D 손실: 0.407124, 정확도: 83.59][G 손실: 3.602556]
    12000 [D 손실: 0.350630, 정확도: 85.16][G 손실: 2.969982]
    13000 [D 손실: 0.298867, 정확도: 87.89][G 손실: 3.664220]
    14000 [D 손실: 0.386743, 정확도: 83.20][G 손실: 2.933915]
    15000 [D 손실: 0.412524, 정확도: 80.86][G 손실: 2.915117]
    16000 [D 손실: 0.496284, 정확도: 71.88][G 손실: 2.267898]
    17000 [D 손실: 0.305741, 정확도: 84.77][G 손실: 2.764291]
    18000 [D 손실: 0.330929, 정확도: 85.94][G 손실: 3.231885]
    19000 [D 손실: 0.531196, 정확도: 76.95][G 손실: 2.451639]
    20000 [D 손실: 0.423364, 정확도: 78.91][G 손실: 1.838722]
    Time elapsed:4862.73 sec 
    Time elapsed:81.05 min 
    


    
![png](output_8_1.png)
    



    
![png](output_8_2.png)
    



    
![png](output_8_3.png)
    



    
![png](output_8_4.png)
    



    
![png](output_8_5.png)
    



    
![png](output_8_6.png)
    



    
![png](output_8_7.png)
    



    
![png](output_8_8.png)
    



    
![png](output_8_9.png)
    



    
![png](output_8_10.png)
    



    
![png](output_8_11.png)
    



    
![png](output_8_12.png)
    



    
![png](output_8_13.png)
    



    
![png](output_8_14.png)
    



    
![png](output_8_15.png)
    



    
![png](output_8_16.png)
    



    
![png](output_8_17.png)
    



    
![png](output_8_18.png)
    



    
![png](output_8_19.png)
    



    
![png](output_8_20.png)
    


### 학습시간(계산시간) 알고리즘


```python
import time
from datetime import timedelta

start = time.process_time()

# sum = 0
# for i in range(10000000):
#     sum += i


end = time.process_time()   
print("Time elapsed:{:.2f} sec " .format(end - start))  # seconds
print("Time elapsed:{:.2f} min " .format((end - start)/60)) # min
```
